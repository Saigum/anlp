{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/saigum/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import polars as po\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from transformers import GPT2Tokenizer\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataChunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class datasetobj:\n",
        "    def __init__(self,df_path):\n",
        "        self.df = po.read_csv(df_path, has_header=False, new_columns=[\"en\", \"fr\"])\n",
        "        self.vocab_en = FreqDist()\n",
        "        self.vocab_fr = FreqDist()\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "    def __len__(self):\n",
        "        return(len(self.df))\n",
        "    def __getitem__(self,index):\n",
        "        en = self.tokenizer(self.df[index,0])[\"input_ids\"]\n",
        "        fr = self.tokenizer(self.df[index,1])[\"input_ids\"]\n",
        "        return en,fr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import random_split\n",
        "class EnFinnishDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,archive_path:str,context_len:int=512):\n",
        "        super().__init__()\n",
        "        with open(os.path.join(archive_path,\"EUbookshop.en\")) as fp:\n",
        "            self.english_corpus = fp.readlines()\n",
        "        with open(os.path.join(archive_path,\"EUbookshop.fi\")) as fp:\n",
        "            self.finnish_corpus = fp.readlines()\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
        "        print(self.tokenizer.pad_token)      \n",
        "        print(self.tokenizer.pad_token_id)     \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        en_tokens = self.tokenizer(self.english_corpus[index],\n",
        "                                   padding=\"max_length\",\n",
        "                                   max_length=512)[\"input_ids\"]\n",
        "        finnish_tokens = self.tokenizer(self.finnish_corpus[index],padding=\"max_length\",max_length=512)[\"input_ids\"]\n",
        "        return (en_tokens,finnish_tokens)\n",
        "\n",
        "@dataclass\n",
        "class DataModuleConfig:\n",
        "    archive_path:str=\"EUbookshop-1\"\n",
        "    batch_size:int=32\n",
        "    train_test:float=0.8\n",
        "    train_val:float=0.8 \n",
        "    context_len:int=512\n",
        "     \n",
        "class EnFinDataModule(lightning.LightningDataModule):\n",
        "    def __init__(self,\n",
        "                 config:DataModuleConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "    def setup(self, stage):\n",
        "        FullDataset = EnFinnishDataset(self.config.archive_path)\n",
        "        self.train_ds,self.valds = random_split(FullDataset,lengths=[self.config.train_test*len(FullDataset),(1-self.config.train_test)*len(FullDataset)])\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_ds,batch_size=self.config.batch_size)\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.train_ds,batch_size=self.config.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad>\n",
            "50257\n",
            "[18467, 1044, 198, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
            "Number of en tokens : 512\n",
            "Number of fi tokens : 512\n",
            "English: Finland\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Finnish: Suomi\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "[464, 3427, 5483, 7557, 287, 17837, 11, 4343, 12, 6390, 198, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
            "Number of en tokens : 512\n",
            "Number of fi tokens : 512\n",
            "English: The European Social Fund in Finland, 2007-2013\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Finnish: Euroopan sosiaalirahasto Suomessa 2007-2013\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "[18467, 1044, 447, 247, 82, 13380, 37, 11383, 481, 1104, 2440, 13714, 287, 262, 16433, 284, 4155, 3744, 1597, 36812, 13, 198, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
            "Number of en tokens : 512\n",
            "Number of fi tokens : 512\n",
            "English: Finland’s ESF programme will support higher productivity in the workforce to ensure greater business competitiveness.\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Finnish: Ohjelmakaudella 2007-20013 Suomen ESR-ohjelma tukee elinkeinoelämän kilpailukykyä ja työllisyyttä.\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "[3633, 3047, 290, 1365, 2842, 286, 1762, 481, 2620, 7184, 13, 198, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
            "Number of en tokens : 512\n",
            "Number of fi tokens : 512\n",
            "English: While training and better ways of working will increase employment.\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Finnish: Työn tuottavuutta parannetaan koulutuksen sekä työorganisaatioiden ja työmarkkinoiden kehittämisen avulla.\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "[464, 1410, 635, 6971, 262, 2478, 286, 760, 12, 4919, 11, 25438, 290, 2594, 11, 290, 21068, 13347, 14900, 287, 262, 10515, 1910, 13, 198, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
            "Number of en tokens : 512\n",
            "Number of fi tokens : 512\n",
            "English: The plan also supports the development of know-how, innovations and services, and promotes sustainable inclusion in the labour market.\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Finnish: ESR-ohjelma tukee myös osaamisen ja innovaatiotoiminnan kehittämistä sekä edistää kestävää työllistymistä ja torjuu syrjäytymistä työmarkkinoilta.\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ],
      "source": [
        "def testDataset(archive:str=\"EUbookshop-1\"):\n",
        "    ds = EnFinnishDataset(archive)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
        "    for i in range(5):\n",
        "        en_tokens,fi_tokens  = ds[i]\n",
        "        print(en_tokens)        \n",
        "        print(f\"Number of en tokens : {len(en_tokens)}\")\n",
        "        print(f\"Number of fi tokens : {len(fi_tokens)}\")\n",
        "        print(f\"English: {tokenizer.decode(en_tokens)}\")\n",
        "        print(f\"Finnish: {tokenizer.decode(fi_tokens)}\")\n",
        "        \n",
        "testDataset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "## learned positional_encoding, like in GPT-2\n",
        "class PositionalEncodings(nn.Module):\n",
        "    def __init__(self,max_seq_len:int,hidden_size:int):\n",
        "        super().__init__()\n",
        "        self.pos_emb = nn.Embedding(num_embeddings=max_seq_len,embedding_dim=hidden_size)\n",
        "    def forward(self,positions):\n",
        "        ## positions would have to be a set of indices\n",
        "        return(self.pos_emb(positions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def rotation_matrix(m:int,context_len:int=512):\n",
        "    thetas = torch.pow(1e4,torch.arange(start=0,end=-context_len+2,step=-2)/context_len)\n",
        "    cosines = torch.cos_(m*thetas)\n",
        "    sines = torch.cos_(m*thetas)\n",
        "    ro_pe = torch.zeros(size=(context_len,context_len))\n",
        "    \n",
        "class RotaryPE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "    def forward(self,positions):\n",
        "        ## positions is an "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class attnconfig:\n",
        "    query_dim:int\n",
        "    key_dim:int\n",
        "    value_dim:int\n",
        "    model_dim:int\n",
        "    n_heads:int\n",
        "    causal_mask:bool=False\n",
        "    \n",
        "    \n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self,config:attnconfig):\n",
        "        super().__init__()\n",
        "        self.Wq = nn.ModuleList([nn.Linear(config.query_dim,config.model_dim//config.n_heads) for _ in range(config.n_heads)])\n",
        "        self.Wk = nn.ModuleList([nn.Linear(config.key_dim,config.model_dim//config.n_heads) for _ in range(config.n_heads)])\n",
        "        self.Wv = nn.ModuleList([nn.Linear(config.value_dim,config.model_dim//config.n_heads) for _ in range(config.n_heads)])\n",
        "        self.sf = nn.Softmax(dim=-1)\n",
        "        self.config=config\n",
        "    def forward(self,query_vector,key_vector,value_vector):\n",
        "        output =[]\n",
        "        for i in range(self.config.n_heads):\n",
        "            q=self.Wq[i](query_vector)\n",
        "            k=self.Wk[i](key_vector)\n",
        "            v=self.Wv[i](value_vector)\n",
        "            A = self.sf(torch.matmul(q,k.T)/(np.sqrt(1/q.shape[1])))\n",
        "            output.append(A@v)\n",
        "            # print(f\"Shape of head {i} output; {output[-1].shape}\")\n",
        "        return(torch.cat(output,dim=-1))\n",
        "\n",
        "class MaskedMultiHeadAttention(MultiHeadedAttention):\n",
        "    def forward(self,query_vector,key_vector,value_vector):\n",
        "        output =[]\n",
        "        for i in range(self.config.n_heads):\n",
        "            q=self.Wq[i](query_vector)\n",
        "            k=self.Wk[i](key_vector)\n",
        "            v=self.Wv[i](value_vector)\n",
        "            A = self.sf(torch.matmul(q,k.T)/(np.sqrt(1/q.shape[1])))\n",
        "            A= torch.tril(input=A)\n",
        "            output.append(A@v)\n",
        "            # print(f\"Shape of head {i} output; {output[-1].shape}\")\n",
        "        return(torch.cat(output,dim=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResMLP(nn.Module):\n",
        "    def __init__(self, input_size:int,num_layers:int):\n",
        "        super().__init__()\n",
        "        self.Linears = nn.ModuleList([nn.Linear(input_size,input_size) for _ in range(num_layers)])\n",
        "    def forward(self,x):\n",
        "        res =x\n",
        "        for i in range(len(self.Linears)):\n",
        "            x = self.Linears[i](x)  \n",
        "        return res+x\n",
        "@dataclass\n",
        "class EncoderConfig:\n",
        "      num_heads:int=4\n",
        "      vocab_size:int=50762\n",
        "      embedding_size:int=768\n",
        "      max_seq_len:int=200\n",
        "      atn_cfg:attnconfig=attnconfig(query_dim=embedding_size,key_dim=embedding_size,value_dim=embedding_size,model_dim=embedding_size,n_heads=num_heads)\n",
        "      pos_weight:int=0.2\n",
        "      mlp_depth:int=1\n",
        "      \n",
        "      \n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self,config:EncoderConfig):\n",
        "        super().__init__()\n",
        "        self.Embedding = nn.Embedding(config.vocab_size,config.embedding_size)\n",
        "        self.PositionalEncoding =  PositionalEncodings(config.max_seq_len,config.embedding_size)\n",
        "        self.attn_head = MultiHeadedAttention(config=config.atn_cfg)\n",
        "        self.layer_norm1 = nn.LayerNorm(config.embedding_size)\n",
        "        self.res1 = ResMLP(input_size=config.embedding_size,num_layers=config.mlp_depth)\n",
        "        self.layer_norm2 = nn.LayerNorm(config.embedding_size)\n",
        "        self.encodercfg = config\n",
        "    \n",
        "    def forward(self,x):\n",
        "        embs = self.Embedding(x)\n",
        "        pos_embs = self.PositionalEncoding(torch.arange(0,end=x.shape[0]))\n",
        "        embs = embs + self.encodercfg.pos_weight*pos_embs\n",
        "        embs = self.layer_norm1(self.attn_head(embs,embs,embs) + embs)\n",
        "        embs = self.layer_norm2(self.res1(embs))\n",
        "        return embs\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass \n",
        "class DecoderConfig:\n",
        "    num_heads:int=4\n",
        "    vocab_size:int=50762\n",
        "    embedding_size:int=768\n",
        "    max_seq_len:int=200\n",
        "    atn_cfg:attnconfig=attnconfig(query_dim=embedding_size,key_dim=embedding_size,value_dim=embedding_size,model_dim=embedding_size,n_heads=num_heads)\n",
        "    pos_weight:int=0.2\n",
        "    mlp_depth:int=1\n",
        "    \n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self,config:DecoderConfig):\n",
        "        super().__init__()\n",
        "        self.Embedding = nn.Embedding(config.vocab_size,config.embedding_size)\n",
        "        self.PositionalEncoding =  PositionalEncodings(config.max_seq_len,config.embedding_size)\n",
        "        self.attn_head = MaskedMultiHeadAttention(config.atn_cfg)\n",
        "        self.res1 = nn.Sequential( ResMLP(input_size=config.embedding_size,num_layers=config.mlp_depth),\n",
        "                                  nn.LayerNorm(config.embedding_size))\n",
        "        self.CrossAttention = MultiHeadedAttention(config.atn_cfg)\n",
        "        \n",
        "        self.fc = nn.Sequential(nn.Linear(config.embedding_size,config.vocab_size),\n",
        "                                nn.Softmax(-1))\n",
        "        self.layer_norm1 = nn.LayerNorm(config.embedding_size)\n",
        "        self.layer_norm2 = nn.LayerNorm(config.embedding_size)\n",
        "        self.layer_norm3 = nn.LayerNorm(config.embedding_size)\n",
        "        self.decodercfg = config\n",
        "        \n",
        "    def forward(self,tokens,encoder_output):\n",
        "        token_embeddings =  self.Embedding(tokens)\n",
        "        pos_embs = self.PositionalEncoding(torch.arange(0,end=tokens.shape[0]))\n",
        "        embs = embs + self.decodercfg.pos_weight*pos_embs\n",
        "        embs = self.layer_norm1(self.attn_head(embs,embs,embs) + embs)\n",
        "\n",
        "        embs = self.layernorm2(embs +  self.CrossAttention(embs,encoder_output,encoder_output))\n",
        "        embs  = self.layer_norm3(self.res1(embs))\n",
        "        return self.fc(embs)\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass \n",
        "class TransformerConfig:\n",
        "    encoder_cfg:EncoderConfig\n",
        "    decoder_cfg:DecoderConfig\n",
        "\n",
        "class Transformer(lightning.LightningModule):\n",
        "    def __init__(self, config:TransformerConfig):\n",
        "        self.encoder =  TransformerEncoderBlock(config.encoder_cfg)\n",
        "        self.decoder = TransformerDecoderBlock(config.decoder_cfg)\n",
        "        self.loss =  torch.nn.CrossEntropyLoss()\n",
        "    def forward(self,eng_tokens,fin_tokens):\n",
        "        encoder_outputs = self.encoder(eng_tokens)\n",
        "        ## need to right shift the tokens.\n",
        "        \n",
        "        decoder_output = self.decoder(fin_tokens,encoder_outputs)\n",
        "        return decoder_output\n",
        "    def training_step(self,batch,batch_idx):\n",
        "        ## need to consider batches.\n",
        "        ## ?\n",
        "        en,fin = batch\n",
        "        \n",
        "        return super().training_step()\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
        "    \n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is my mha op shape: torch.Size([5, 20])\n",
            "This is torch mha output shape torch.Size([5, 20])\n",
            "This is my mha op shape: torch.Size([5, 20])\n",
            "This is torch mha output shape torch.Size([5, 20])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "multihead_config = attnconfig(query_dim=20,key_dim=20,value_dim=20,model_dim=20,n_heads=2)\n",
        "selfattn_config = attnconfig(query_dim=20,key_dim=20,value_dim=20,model_dim=20,n_heads=1)\n",
        "\n",
        "def test_mha(config:attnconfig=selfattn_config):\n",
        "    mha_layer = MultiHeadedAttention(config)\n",
        "    torch_mha = nn.MultiheadAttention(config.model_dim,config.n_heads)\n",
        "    N_vectors = 5\n",
        "    query_vector = torch.randn([N_vectors,config.query_dim])\n",
        "    key_vector = torch.randn([N_vectors,config.key_dim])\n",
        "    value_vector = torch.randn([N_vectors,config.value_dim])\n",
        "    with torch.no_grad():\n",
        "        output = mha_layer(query_vector,key_vector,value_vector)\n",
        "        torchput,attn_weights = torch_mha(query_vector,key_vector,value_vector)\n",
        "    print(f\"This is my mha op shape: {output.shape}\") ## should be n x value vector size\n",
        "    print(f\"This is torch mha output shape {torchput.shape}\")\n",
        "\n",
        "test_mha(selfattn_config)\n",
        "test_mha(multihead_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 768])\n"
          ]
        }
      ],
      "source": [
        "encfg=EncoderConfig(num_heads=4,vocab_size=50762,embedding_size=768,max_seq_len=2000,pos_weight=0.2)\n",
        "def test_encoder(encodercfg:EncoderConfig):\n",
        "    trans = TransformerEncoderBlock(encodercfg)\n",
        "    batch_size=10\n",
        "    input_tokens = torch.randint(low=0,high=encodercfg.vocab_size,size=(batch_size,))\n",
        "    with torch.no_grad():\n",
        "        output = trans(input_tokens)\n",
        "    print(output.shape)\n",
        "test_encoder(encfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of theta matrix is : torch.Size([40, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, embedding_dim:int,context_len:int):\n",
        "        super().__init__( )\n",
        "        self.d = embedding_dim\n",
        "        thetas = torch.arange(start=0,end=context_len,step=1,dtype=torch.float).view(-1,1) @torch.pow(1e5,-2*torch.arange(start=0,end=self.d-1,step=2)/self.d).repeat_interleave(2).view(1,-1)\n",
        "        ## this should be an context_len x d size matrix \n",
        "        print(f\"Shape of theta matrix is : {thetas.shape}\")\n",
        "        self.costhetas  = torch.cos(thetas)\n",
        "        self.sinethetas = torch.sin(thetas)\n",
        "        self.even_idx = torch.arange(start=0,end=self.d,step=2,dtype=torch.int)\n",
        "        self.odd_idx = torch.arange(start=1,end=self.d,step=2,dtype=torch.int)\n",
        "\n",
        "    def interswap(self,token_embedding):\n",
        "        odds =  token_embedding[...,self.odd_idx]\n",
        "        evens = token_embedding[...,self.even_idx]\n",
        "        token_embedding[...,self.odd_idx] =  -1*evens\n",
        "        token_embedding[...,self.even_idx] = odds\n",
        "        return token_embedding\n",
        "    \n",
        "    def forward(self,token_embeddings):\n",
        "        output = token_embeddings*self.costhetas + self.interswap(token_embeddings)*self.sinethetas\n",
        "        return output\n",
        "\n",
        "\n",
        "class RelativePE(nn.Module):\n",
        "    def __init__(self, embedding_dim:int,context_len:int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings=context_len,embedding_dim=embedding_dim)\n",
        "        \n",
        "        \n",
        "def test_rope():\n",
        "    batch_dim=32\n",
        "    num_tokens=40\n",
        "    embedding_dim=512\n",
        "    token_embedding = torch.randn(size=(batch_dim,num_tokens,embedding_dim))\n",
        "    rope = RoPE(embedding_dim=embedding_dim,context_len=num_tokens)\n",
        "    output = rope(token_embedding)\n",
        "     \n",
        "test_rope()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.arange(10).view(1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /home/saigum/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
            "loading file merges.txt from cache at /home/saigum/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /home/saigum/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at /home/saigum/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /home/saigum/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.54.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size pre-addition: 50257\n",
            "Vocab size pre-addition: 50258\n",
            "<pad>\n",
            "50257\n",
            "This is the vocabulary size: 50257\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "from transformers import GPT2Tokenizer\n",
        "import os\n",
        "import numpy as np \n",
        "\n",
        "ARCHIVE_PATH = \"EUbookshop-1\"\n",
        "CONTEXT_LEN = 512\n",
        "\n",
        "\n",
        "class EnFinnishDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,archive_path:str,context_len:int=512):\n",
        "        super().__init__()\n",
        "        with open(os.path.join(archive_path,\"EUbookshop.en\")) as fp:\n",
        "            self.english_corpus = fp.readlines()\n",
        "        with open(os.path.join(archive_path,\"EUbookshop.fi\")) as fp:\n",
        "            self.finnish_corpus = fp.readlines()\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "        print(f\"Vocab size pre-addition: {len(self.tokenizer)}\")\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
        "        self.context_len = context_len\n",
        "        print(f\"Vocab size pre-addition: {len(self.tokenizer)}\")\n",
        "        \n",
        "        print(self.tokenizer.pad_token)      \n",
        "        print(self.tokenizer.pad_token_id)     \n",
        "\n",
        "    def return_masks(self,pad_idx:int):\n",
        "        pad_masks = torch.zeros(size=(self.context_len,self.context_len))\n",
        "        pad_masks[:,pad_idx:] = -torch.inf\n",
        "        pad_masks[pad_idx:,:] = -torch.inf\n",
        "        return pad_masks\n",
        "    def __getitem__(self, index):\n",
        "        en_tokens = torch.tensor(self.tokenizer(self.english_corpus[index],padding=\"max_length\",max_length=self.context_len)[\"input_ids\"])\n",
        "        finnish_tokens = torch.tensor(self.tokenizer(self.finnish_corpus[index],padding=\"max_length\",max_length=self.context_len)[\"input_ids\"])\n",
        "        en_pad_index = torch.where(en_tokens==self.tokenizer.pad_token_id)[0][0]\n",
        "        fin_pad_index = torch.where(finnish_tokens == self.tokenizer.pad_token_id)[0][0]\n",
        "        en_pad_masks = self.return_masks(en_pad_index)\n",
        "        fin_pad_masks = self.return_masks(fin_pad_index)\n",
        "        # en_pad_masks = torch.concat([torch.full((en_pad_index,),fill_value=0),torch.full((self.context_len-en_pad_index,),-torch.inf)])\n",
        "        # fin_pad_masks = torch.concat([torch.full((fin_pad_index,),fill_value=0),torch.full((self.context_len-fin_pad_index,),-1*torch.inf)])\n",
        "        # en_pad_masks = en_pad_masks.view(-1,1)@torch.concat([torch.zeros((fin_pad_index,)),torch.ones(self.context_len-fin_pad_index)])\n",
        "        # print(en_pad_masks)\n",
        "        return (en_tokens,en_pad_masks,finnish_tokens,fin_pad_masks)\n",
        "    def __len__(self):\n",
        "        return len(self.english_corpus)\n",
        "\n",
        "\n",
        "def test_dataloading_and_item_shape():\n",
        "    \"\"\"\n",
        "    Tests the instantiation of the EnFinnishDataset and verifies the structure,\n",
        "    type, and shape of a single item fetched from it.\n",
        "    \"\"\"\n",
        "    dataset = EnFinnishDataset(archive_path=ARCHIVE_PATH, context_len=CONTEXT_LEN)\n",
        "    print(f\"This is the vocabulary size: {dataset.tokenizer.vocab_size}\")\n",
        "    # 1. Assert that the dataset object is created and is not empty.\n",
        "    assert dataset is not None, \"Dataset object could not be instantiated.\"\n",
        "    assert len(dataset) > 0, \"Dataset is empty after loading.\"\n",
        "\n",
        "    # 2. Retrieve a single sample to check its integrity.\n",
        "    sample = dataset[0]\n",
        "    assert isinstance(sample, tuple) and len(sample) == 4, f\"Dataset sample should be a tuple of 4 elements, but got {type(sample)} of length {len(sample)}.\"\n",
        "    \n",
        "    en_tokens, en_mask, fin_tokens, fin_mask = sample\n",
        "    \n",
        "    # 3. Assert that all parts of the sample are tensors with the correct shape.\n",
        "    expected_shape = [torch.Size([CONTEXT_LEN]),torch.Size([CONTEXT_LEN,CONTEXT_LEN])]*2\n",
        "    checks=[(\"English tokens\", en_tokens), (\"English mask\", en_mask), (\"Finnish tokens\", fin_tokens), (\"Finnish mask\", fin_mask)]\n",
        "    for i,(name, tensor) in enumerate(checks):\n",
        "        assert isinstance(tensor, torch.Tensor), f\"{name} is not a torch.Tensor.\"\n",
        "        assert tensor.shape == expected_shape[i], (\n",
        "            f\"{name} shape is incorrect.\\n\"\n",
        "            f\"Expected: {expected_shape}, Got: {tensor.shape}\"\n",
        "        )\n",
        "\n",
        "test_dataloading_and_item_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class EnFinnishDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,archive_path:str,context_len:int=512):\n",
        "        super().__init__()\n",
        "        with open(os.path.join(archive_path,\"EUbookshop.en\")) as fp:\n",
        "            self.english_corpus = fp.readlines()\n",
        "        with open(os.path.join(archive_path,\"EUbookshop.fi\")) as fp:\n",
        "            self.finnish_corpus = fp.readlines()\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '<pad>',\"bos_token\": \"<bos>\"})        \n",
        "        print(\"PAD token:\", self.tokenizer.pad_token, self.tokenizer.pad_token_id)\n",
        "        print(\"EOS token:\", self.tokenizer.eos_token, self.tokenizer.eos_token_id)\n",
        "        print(\"BOS token:\", self.tokenizer.bos_token, self.tokenizer.bos_token_id)\n",
        "        self.context_len = context_len\n",
        "        print(self.tokenizer.pad_token)      \n",
        "        print(self.tokenizer.pad_token_id)     \n",
        "\n",
        "    def return_masks(self,pad_idx:int):\n",
        "        pad_masks = torch.zeros(size=(self.context_len,self.context_len))\n",
        "        pad_masks[:,pad_idx:] = -1e9\n",
        "        pad_masks[pad_idx:,:] = -1e9\n",
        "        return pad_masks\n",
        "    def __getitem__(self, index):\n",
        "        en_tokens = torch.tensor(self.tokenizer(self.english_corpus[index],padding=\"max_length\",max_length=self.context_len)[\"input_ids\"])\n",
        "        finnish_tokens = torch.tensor(self.tokenizer(self.finnish_corpus[index],padding=\"max_length\",max_length=self.context_len)[\"input_ids\"])\n",
        "        en_pad_index = torch.where(en_tokens==self.tokenizer.pad_token_id)[0][0]\n",
        "        fin_pad_index = torch.where(finnish_tokens == self.tokenizer.pad_token_id)[0][0]\n",
        "        en_pad_masks = self.return_masks(en_pad_index)\n",
        "        fin_pad_masks = self.return_masks(fin_pad_index)\n",
        "        # en_pad_masks = torch.concat([torch.full((en_pad_index,),fill_value=0),torch.full((self.context_len-en_pad_index,),-torch.inf)])\n",
        "        # fin_pad_masks = torch.concat([torch.full((fin_pad_index,),fill_value=0),torch.full((self.context_len-fin_pad_index,),-1*torch.inf)])\n",
        "        # en_pad_masks = en_pad_masks.view(-1,1)@torch.concat([torch.zeros((fin_pad_index,)),torch.ones(self.context_len-fin_pad_index)])\n",
        "        # print(en_pad_masks)\n",
        "        return (en_tokens,en_pad_masks,finnish_tokens,fin_pad_masks)\n",
        "    def __len__(self):\n",
        "        return len(self.english_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Dataloading and Shape Sanity Check ---\n",
            "Tokenizer loaded. Pad token: '<pad>', ID: 50257\n",
            "Total vocabulary size: 50259\n",
            "✅ Dataset instantiated and is not empty.\n",
            "✅ Sample is a tuple of 4 elements.\n",
            "✅ All sample tensors have the correct shapes.\n",
            "--- Sanity Check Passed ---\n",
            "\n",
            "Tokenizer loaded. Pad token: '<pad>', ID: 50257\n",
            "Total vocabulary size: 50259\n",
            "\n",
            "--- Starting Full Dataset Analysis ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyzing Dataset: 100%|██████████| 100000/100000 [03:28<00:00, 478.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Analysis Complete ---\n",
            "\n",
            "--- Token Distribution Statistics ---\n",
            "\n",
            "Top 15 Most Common English Tokens:\n",
            "  - Token: ' the' (ID: 262) | Count: 166,399\n",
            "  - Token: ',' (ID: 11) | Count: 108,079\n",
            "  - Token: '\n",
            "' (ID: 198) | Count: 100,000\n",
            "  - Token: ' of' (ID: 286) | Count: 93,319\n",
            "  - Token: '.' (ID: 13) | Count: 92,352\n",
            "  - Token: ' to' (ID: 284) | Count: 71,732\n",
            "  - Token: ' and' (ID: 290) | Count: 61,473\n",
            "  - Token: ' in' (ID: 287) | Count: 48,820\n",
            "  - Token: ' a' (ID: 257) | Count: 37,242\n",
            "  - Token: ' is' (ID: 318) | Count: 35,106\n",
            "  - Token: ' that' (ID: 326) | Count: 33,866\n",
            "  - Token: ' for' (ID: 329) | Count: 25,346\n",
            "  - Token: ' on' (ID: 319) | Count: 23,954\n",
            "  - Token: ' be' (ID: 307) | Count: 21,304\n",
            "  - Token: ' this' (ID: 428) | Count: 16,531\n",
            "\n",
            "Top 15 Most Common Finnish Tokens:\n",
            "  - Token: 'ä' (ID: 11033) | Count: 536,280\n",
            "  - Token: ' k' (ID: 479) | Count: 130,446\n",
            "  - Token: ',' (ID: 11) | Count: 125,960\n",
            "  - Token: 'i' (ID: 72) | Count: 105,696\n",
            "  - Token: '\n",
            "' (ID: 198) | Count: 99,995\n",
            "  - Token: '.' (ID: 13) | Count: 95,249\n",
            "  - Token: 'en' (ID: 268) | Count: 87,774\n",
            "  - Token: 'u' (ID: 84) | Count: 81,960\n",
            "  - Token: 'a' (ID: 64) | Count: 69,357\n",
            "  - Token: ' t' (ID: 256) | Count: 67,479\n",
            "  - Token: 'ks' (ID: 591) | Count: 65,976\n",
            "  - Token: 'is' (ID: 271) | Count: 60,673\n",
            "  - Token: 'ö' (ID: 9101) | Count: 59,243\n",
            "  - Token: 'n' (ID: 77) | Count: 59,070\n",
            "  - Token: 'in' (ID: 259) | Count: 57,754\n",
            "\n",
            "\n",
            "--- Tokenization Integrity Check ---\n",
            "✅ Found 5 mismatched decodings out of 200,000 total sentences.\n",
            "\n",
            "Displaying up to 5 examples of mismatches:\n",
            "Note: Minor differences (e.g., whitespace, normalization) are common.\n",
            "--------------------\n",
            "Language: fi, Index: 2033\n",
            "Original: 'Eurooppalaisille yhteishankkeille annettava tuki voidaan antaa asianomaisten laitosten erityistarpeiden ja vahvistettujen ensisijaisten tavoitteiden mukaista toimintaa varten, mukaan luettuna: i) yhteiset koulutustoimet, erityisesti opetussuunnitelmien kehittämiseksi ja ajanmukaistamiseksi, korkeakoulujen kapasiteetin kehittäminen täydennyskoulutuksen ja uudelleen koulutuksen aloilla, lyhyiden intensiivikurssien kehittäminen ja avointen ja etäopetusjärjestelmien kehittäminen, mukaan lukien tietotekniikka ja viestintätekniikka; ii) korkeakouluopetuksen ja sen kapasiteetin uudistuksen ja kehittämisen hyväksi toteutettavat toimenpiteet, erityisesti korkeakoulujen ja korkeakoulujärjestelmien hallinnon rakenteiden uudistamiseksi nykyaikaistamalla infrastruktuureja, hankkimalla eurooppalaisen yhteishankkeen toteuttamisessa tarvittava välineistö ja tarvittaessa antamalla teknistä ja taloudellista apua vastuullisille viranomaisille; iii) edellä 4 artiklassa määriteltyjen korkeakoulujen, teollisuuden ja laitosten välisen yhteistyön edistäminen eurooppalaisten yhteishankkeiden avulla; iv) opettajien, korkeakoulujen hallintohenkilöstön ja opiskelijoiden liikkuvuuden kehittäminen osana eurooppalaista yhteishanketta: a) apurahoja myönnetään jäsenvaltioiden korkeakoulujen opetus- ja hallintohenkilöstölle tai yritysten kouluttajille aina yhteen vuoteen asti kestävien opetus- ja koulutustehtävien suorittamiseen tukikelpoisissa maissa ja päinvastoin; b) apurahoja myönnetään tukikelpoisten maiden opetus- ja hallintohenkilöstölle uudelleenkoulutukseen ja ajan tasalle saattamiseen Euroopan yhteisössä; c) apurahoja myönnetään opiskelijoille aina tohtoritasoon asti, ja ne suunnataan niin tukikelpoisten maiden opiskelijoille, jotka opiskelevat tietyn ajan Euroopan yhteisössä, kuin yhteisön opiskelijoille, jotka opiskelevat tietyn ajan tukikelpoisissa maissa.'\n",
            "Decoded:  'Eurooppalaisille yhteishankkeille annettava tuki voidaan antaa asianomaisten laitosten erityistarpeiden ja vahvistettujen ensisijaisten tavoitteiden mukaista toimintaa varten, mukaan luettuna: i) yhteiset koulutustoimet, erityisesti opetussuunnitelmien kehittämiseksi ja ajanmukaistamiseksi, korkeakoulujen kapasiteetin kehittäminen täydennyskoulutuksen ja uudelleen koulutuksen aloilla, lyhyiden intensiivikurssien kehittäminen ja avointen ja etäopetusjärjestelmien kehittäminen, mukaan lukien tietotekniikka ja viestintätekniikka; ii) korkeakouluopetuksen ja sen kapasiteetin uudistuksen ja kehittämisen hyväksi toteutettavat toimenpiteet, erityisesti korkeakoulujen ja korkeakoulujärjestelmien hallinnon rakenteiden uudistamiseksi nykyaikaistamalla infrastruktuureja, hankkimalla eurooppalaisen yhteishankkeen toteuttamisessa tarvittava välineistö ja tarvittaessa antamalla teknistä ja taloudellista apua vastuullisille viranomaisille; iii) edellä 4 artiklassa määriteltyjen korkeakoulujen, teollisuuden ja laitosten välisen yhteistyön edistäminen eurooppalaisten yhteishankkeiden avulla; iv) opettajien, korkeakoulujen hallintohenkilöstön ja opiskelijoiden liikkuvuuden kehittäminen osana eurooppalaista yhteishanketta: a) ap'\n",
            "--------------------\n",
            "--------------------\n",
            "Language: fi, Index: 6986\n",
            "Original: 'passitusrikollisten kohteeksi joutuu useimmiten savukekauppa; siinä käytetään hienostuneita taloudellisia toimintoja ja sitä helpottavat tiettyjen EU:n ulkopuolisten maiden (kuten Sveitsi ja Kypros) lainsäädäntö erityisesti pankkisalaisuuden osalta sekä veroparatiiseista käsin toimivat yhtiöt, yhtenäismarkkinoiden perustamista on seurannut tullilaitosten henkilöstömäärien ja motivaation laskuja niiden toisistaan poikkeavat valtuudet, oikeudelliset esteet, perinteet ja tulevaisuuden näkymät ovat haitanneet tehokasta yhteistyötä, pyrkiminen passitusalan petosten hallintaan vakuusjärjestelmää tiukentamalla ei ole estänyt rikollisuutta vaan \"siirtänyt\" sen tavanomaisempien tullirikosten piiriin, passitusrikoksiin liittyvä tehokas syytteeseenpano yli rajojen on äärimmäisen vaikeaa ja sen yhteydessä on selvitettävä ylenpalttisen byrokraattisia menettelyjä, kansallisten lainsäädäntöjen välisiä eroja ja poikkeavia todistusstandardeja; erityisongelmia liittyy Sveitsiin, joka antaa hyvin rajoitettua oikeudellista apua muille maille verorikkomusten alalla, vastuuta nykyisestä asiaintilasta ei voida sälyttää yhden henkilön tai toimielimen harteille, vaan se jakautuu kaikkien järjestelmän lainsäätäjien ja hoitajien kesken.'\n",
            "Decoded:  'passitusrikollisten kohteeksi joutuu useimmiten savukekauppa; siinä käytetään hienostuneita taloudellisia toimintoja ja sitä helpottavat tiettyjen EU:n ulkopuolisten maiden (kuten Sveitsi ja Kypros) lainsäädäntö erityisesti pankkisalaisuuden osalta sekä veroparatiiseista käsin toimivat yhtiöt, yhtenäismarkkinoiden perustamista on seurannut tullilaitosten henkilöstömäärien ja motivaation laskuja niiden toisistaan poikkeavat valtuudet, oikeudelliset esteet, perinteet ja tulevaisuuden näkymät ovat haitanneet tehokasta yhteistyötä, pyrkiminen passitusalan petosten hallintaan vakuusjärjestelmää tiukentamalla ei ole estänyt rikollisuutta vaan \"siirtänyt\" sen tavanomaisempien tullirikosten piiriin, passitusrikoksiin liittyvä tehokas syytteeseenpano yli rajojen on äärimmäisen vaikeaa ja sen yhteydessä on selvitettävä ylenpalttisen byrokraattisia menettelyjä, kansallisten lainsäädäntöjen välisiä eroja ja poikkeavia todistusstandardeja; erityisongelmia liittyy Sveitsiin, joka antaa hyvin rajoitettua oikeudellista apua muille maille verorikkomusten alalla, vastuuta nykyisestä asiaintilasta ei voida sälyttää yhden henkilön tai toimielimen harteille, vaan se jakautuu kaikkien järjestelmän l'\n",
            "--------------------\n",
            "--------------------\n",
            "Language: fi, Index: 12809\n",
            "Original: 'FABIO GHISELLL FABIO MARCUS GHISELLI, GARY RICHARDSON, KENNETH RICHARD STEVENS, STUART ALLAN MacTAGGART, MARK ANTHONY BENNETT ja PETER ALAN LISSENBURGH ovat syyskuun 26. päivän 1993 ja maaliskuun 10. päivän 1994 välisenä aikana a) järjestämällä Bristolissa sijaitsevan tullivaraston käyttämisen b) järjestämällä 1.180:n verovapaita savukkeita sisältävän laatikon toimittamisen kyseiseen tullivarastoon c) järjestämällä näiden 1180 savukkeita sisältävän laatikon lähettämisen tullisinetöityinä Bristolista kohti Marokkoa d) järjestämällä näiden savukkeiden kuljettamisen tullisinetöityinä Bristolista Doveriin ja Ranskaan menevälle lautalle, e) hankkimalla ja toimittamalla kuljetusasiakirjoja, tarkemmin sanottuna laskun 94010/093, carnet'n 10084583 sekä asiakirjat C88 02/080 ja CMR SF851169, näitä savukkeita varten, f) hankkimalla väärennetyn leiman ja leimaamalla irroiteosan nro 1 carnet'sta 10084583 osoittamaan kuorma-auton ja savukkeiden saapumisen Algecirasiin 5.2.1994, osallistuneet tai yllyttäneet rikokseen, tarkemmin sanottuna tullattavien tavaroiden (1188:n verovapaita savukkeita sisältävän, yhteensä yli 1.000.000 pesetan arvoisen laatikon) tuontiin Espanjaan maksamatta veroja ja niiden myyntiin siellä maksamatta veroja, Espanjan lain, tarkemmin sanottuna Espanjan rikoslain 1.1, 12, 349 ja 350 artiklojen, määräysten vastaisesti.'\n",
            "Decoded:  'FABIO GHISELLL FABIO MARCUS GHISELLI, GARY RICHARDSON, KENNETH RICHARD STEVENS, STUART ALLAN MacTAGGART, MARK ANTHONY BENNETT ja PETER ALAN LISSENBURGH ovat syyskuun 26. päivän 1993 ja maaliskuun 10. päivän 1994 välisenä aikana a) järjestämällä Bristolissa sijaitsevan tullivaraston käyttämisen b) järjestämällä 1.180:n verovapaita savukkeita sisältävän laatikon toimittamisen kyseiseen tullivarastoon c) järjestämällä näiden 1180 savukkeita sisältävän laatikon lähettämisen tullisinetöityinä Bristolista kohti Marokkoa d) järjestämällä näiden savukkeiden kuljettamisen tullisinetöityinä Bristolista Doveriin ja Ranskaan menevälle lautalle, e) hankkimalla ja toimittamalla kuljetusasiakirjoja, tarkemmin sanottuna laskun 94010/093, carnet'n 10084583 sekä asiakirjat C88 02/080 ja CMR SF851169, näitä savukkeita varten, f) hankkimalla väärennetyn leiman ja leimaamalla irroiteosan nro 1 carnet'sta 10084583 osoittamaan kuorma-auton ja savukkeiden saapumisen Algecirasiin 5.2.1994, osallistuneet tai yllyttäneet rikokseen, tarkemmin sanottuna tullattavien tavaroiden (1188:n verovapaita savukkeita sisältävän, yhteensä yli 1.000.000 peset'\n",
            "--------------------\n",
            "--------------------\n",
            "Language: fi, Index: 12812\n",
            "Original: 'FABIO GFflSELLL FABIO MARCUS GFflSELLL GARY RICHARDSON, KENNETH RICHARD STEVENS, STUART ALLAN MacTAGGART, MARK ANTHONY BENNETT ja PETER ALAN LISSENBURGH ovat syyskuun 26. päivän 1993 ja toukokuun 10. päivän 1994 välisenä aikana a) järjestämällä Bristolissa sijaitsevan tullivaraston käyttämisen b) järjestämällä 1.180:n verovapaita savukkeita sisältävän laatikon toimittamisen kyseiseen tullivarastoon c) järjestämällä näiden 1180 savukkeita sisältävän laatikon lähettämisen tullisinetöityinä Bristolista kohti Marokkoa d) järjestämällä näiden savukkeiden kuljettamisen tullisinetöityinä Bristolista Doveriin ja Ranskaan menevälle lautalle, e) hankkimalla ja toimittamalla kuljetusasiakirjoja, tarkemmin sanottuna laskun 9403/093, camet'n 10084584 ja asiakirjat C88 02/052 sekä CMR näitä savukkeita varten, f) hankkimalla väärennetyn leiman ja leimaamalla kantaosan (Souche) nro 2 carnet'n 10084584 sivulla 2 osoittamaan kuorma-auton ja savukkeiden saapuneen Algecirasiin 22.2.1994, g) palauttamalla väärennetyllä leimalla varustettu carnet 10084584 FT A: han 31.3.1994 osallistuneet tai yllyttäneet rikokseen, tarkemmin sanottuna tullattavien tavaroiden (1155 laatikollisen verovapaita savukkeita, joiden arvo oli yli 1.000.000 pesetaa) tuomiseen Espanjaan maksamatta veroja ja niiden myymiseen Espanjassa maksamatta veroja, rikkoen Espanjan lakia, tarkemmin sanottuna Espanjan rikoslain 1.1, 12, 349 ja 350 artikloja.'\n",
            "Decoded:  'FABIO GFflSELLL FABIO MARCUS GFflSELLL GARY RICHARDSON, KENNETH RICHARD STEVENS, STUART ALLAN MacTAGGART, MARK ANTHONY BENNETT ja PETER ALAN LISSENBURGH ovat syyskuun 26. päivän 1993 ja toukokuun 10. päivän 1994 välisenä aikana a) järjestämällä Bristolissa sijaitsevan tullivaraston käyttämisen b) järjestämällä 1.180:n verovapaita savukkeita sisältävän laatikon toimittamisen kyseiseen tullivarastoon c) järjestämällä näiden 1180 savukkeita sisältävän laatikon lähettämisen tullisinetöityinä Bristolista kohti Marokkoa d) järjestämällä näiden savukkeiden kuljettamisen tullisinetöityinä Bristolista Doveriin ja Ranskaan menevälle lautalle, e) hankkimalla ja toimittamalla kuljetusasiakirjoja, tarkemmin sanottuna laskun 9403/093, camet'n 10084584 ja asiakirjat C88 02/052 sekä CMR näitä savukkeita varten, f) hankkimalla väärennetyn leiman ja leimaamalla kantaosan (Souche) nro 2 carnet'n 10084584 sivulla 2 osoittamaan kuorma-auton ja savukkeiden saapuneen Algecirasiin 22.2.1994, g) palauttamalla väärennetyllä leimalla varustettu carnet 10084584 FT A: han 31.3.1994 osallistuneet tai yllyttäneet rikokseen, tarkemmin sanottuna tullattavien t'\n",
            "--------------------\n",
            "--------------------\n",
            "Language: fi, Index: 23949\n",
            "Original: 'Me Alleanza Nazionale - puolueen jäsenet pidämme edessä olevia kuukausia erityisen merkittävinä, koska silloin on saatava päätökseen hallitusten välinen konferenssi ja löydettävä vihdoinkin avaimet visioihin tulevaisuuden Euroopan unionista. Tuleeko se olemaan Euroopan unioni \"a la carte\", niin kuin sanotaan, jossa jokainen maa saa sitä, mikä sitä eniten kiinnostaa tuntematta itseään velvoitetuksi tehtäviin, jotka eivät sille kuulu; tai yhtenäismarkkinoiden ja yhtenäisvaluutan Eurooppa, jossa yksittäisten maiden raha- ja talouspolitiikkaa säätelee Euroopan keskuspankki ja sen sisällä pieni ryhmä niin sanottuun \"markan alueeseen\" kuuluvia pankkiireja; tai sitten Euroopan unioni, johon kuuluisi myös itä, jolloin se murenisi täysin ellei sääntöjä muutettaisi ja se olisi täynnä työllisyys- ja sosiaaliongelmia, jotka rajoittaisivat sen toimin taa sekä häiritsisivät sen aloitteita muilla alueilla; tai niin kuin me Alleanza Nazionale - puolueessa toivomme, Eurooppa, jolla taloudellisen ja rahoituksellisen yhteen kuuluvuuden lisäksi olisi myös vain yksi kanta ulkopolitiikasta ja turvallisuudesta, Eurooppa, joka pitää elintärkeänä omien etujensa ja olemassaolonsa kannalta Välimeren alueen ongelmia, ja varsinkin räjähdysmäistä väestönkasvua Pohjois-Afrikan rannikolla, mistä on seurauksena epätoivoisten ihmisten laiton maahanmuutto työn ja elämän perässä.'\n",
            "Decoded:  'Me Alleanza Nazionale - puolueen jäsenet pidämme edessä olevia kuukausia erityisen merkittävinä, koska silloin on saatava päätökseen hallitusten välinen konferenssi ja löydettävä vihdoinkin avaimet visioihin tulevaisuuden Euroopan unionista. Tuleeko se olemaan Euroopan unioni \"a la carte\", niin kuin sanotaan, jossa jokainen maa saa sitä, mikä sitä eniten kiinnostaa tuntematta itseään velvoitetuksi tehtäviin, jotka eivät sille kuulu; tai yhtenäismarkkinoiden ja yhtenäisvaluutan Eurooppa, jossa yksittäisten maiden raha- ja talouspolitiikkaa säätelee Euroopan keskuspankki ja sen sisällä pieni ryhmä niin sanottuun \"markan alueeseen\" kuuluvia pankkiireja; tai sitten Euroopan unioni, johon kuuluisi myös itä, jolloin se murenisi täysin ellei sääntöjä muutettaisi ja se olisi täynnä työllisyys- ja sosiaaliongelmia, jotka rajoittaisivat sen toimin taa sekä häiritsisivät sen aloitteita muilla alueilla; tai niin kuin me Alleanza Nazionale - puolueessa toivomme, Eurooppa, jolla taloudellisen ja rahoituksellisen yhteen kuuluvuuden lisäksi olisi myös vain yksi kanta ulkopolitiikasta ja turvallisuudesta, Eurooppa, joka pitää elintärkeänä omien etujensa ja olemassaolonsa kann'\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "from transformers import GPT2Tokenizer\n",
        "import os\n",
        "import numpy as np \n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "ARCHIVE_PATH = \"EUbookshop-1\"\n",
        "CONTEXT_LEN = 512\n",
        "\n",
        "\n",
        "class EnFinnishDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,archive_path:str,context_len:int=512):\n",
        "        super().__init__()\n",
        "        # --- Make sure the files exist before proceeding ---\n",
        "        en_path = os.path.join(archive_path, \"EUbookshop.en\")\n",
        "        fi_path = os.path.join(archive_path, \"EUbookshop.fi\")\n",
        "        if not (os.path.exists(en_path) and os.path.exists(fi_path)):\n",
        "            raise FileNotFoundError(\n",
        "                f\"Could not find dataset files at '{en_path}' and '{fi_path}'. \"\n",
        "                \"Please ensure the archive is extracted correctly.\"\n",
        "            )\n",
        "\n",
        "        with open(en_path, 'r', encoding='utf-8') as fp:\n",
        "            self.english_corpus = fp.readlines()\n",
        "        with open(fi_path, 'r', encoding='utf-8') as fp:\n",
        "            self.finnish_corpus = fp.readlines()\n",
        "        \n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '<pad>',\"bos_token\": \"<bos>\"})        \n",
        "        self.context_len = context_len\n",
        "        \n",
        "        print(f\"Tokenizer loaded. Pad token: '{self.tokenizer.pad_token}', ID: {self.tokenizer.pad_token_id}\")\n",
        "        print(f\"Total vocabulary size: {len(self.tokenizer)}\")\n",
        "\n",
        "    def return_masks(self,pad_idx:int):\n",
        "        # This function creates a square attention mask.\n",
        "        # It's used to prevent the model from attending to padding tokens.\n",
        "        pad_masks = torch.zeros(size=(self.context_len,self.context_len))\n",
        "        if pad_idx < self.context_len: # Check if padding exists\n",
        "            pad_masks[:,pad_idx:] = -torch.inf\n",
        "            pad_masks[pad_idx:,:] = -torch.inf\n",
        "        return pad_masks\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # Tokenize with padding to max_length\n",
        "        en_encoding = self.tokenizer(self.english_corpus[index], padding=\"max_length\", max_length=self.context_len, truncation=True)\n",
        "        fi_encoding = self.tokenizer(self.finnish_corpus[index], padding=\"max_length\", max_length=self.context_len, truncation=True)\n",
        "\n",
        "        en_tokens = torch.tensor(en_encoding[\"input_ids\"])\n",
        "        finnish_tokens = torch.tensor(fi_encoding[\"input_ids\"])\n",
        "\n",
        "        # Find the first occurrence of the pad token to determine sequence length\n",
        "        en_pad_indices = torch.where(en_tokens == self.tokenizer.pad_token_id)[0]\n",
        "        en_pad_index = en_pad_indices[0] if len(en_pad_indices) > 0 else self.context_len\n",
        "\n",
        "        fin_pad_indices = torch.where(finnish_tokens == self.tokenizer.pad_token_id)[0]\n",
        "        fin_pad_index = fin_pad_indices[0] if len(fin_pad_indices) > 0 else self.context_len\n",
        "\n",
        "        en_pad_masks = self.return_masks(en_pad_index)\n",
        "        fin_pad_masks = self.return_masks(fin_pad_index)\n",
        "        \n",
        "        return (en_tokens, en_pad_masks, finnish_tokens, fin_pad_masks)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_corpus)\n",
        "\n",
        "\n",
        "def test_dataloading_and_item_shape():\n",
        "    \"\"\"\n",
        "    Tests the instantiation of the EnFinnishDataset and verifies the structure,\n",
        "    type, and shape of a single item fetched from it.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Running Dataloading and Shape Sanity Check ---\")\n",
        "    dataset = EnFinnishDataset(archive_path=ARCHIVE_PATH, context_len=CONTEXT_LEN)\n",
        "    \n",
        "    # 1. Assert that the dataset object is created and is not empty.\n",
        "    assert dataset is not None, \"Dataset object could not be instantiated.\"\n",
        "    assert len(dataset) > 0, \"Dataset is empty after loading.\"\n",
        "    print(\"✅ Dataset instantiated and is not empty.\")\n",
        "\n",
        "    # 2. Retrieve a single sample to check its integrity.\n",
        "    sample = dataset[0]\n",
        "    assert isinstance(sample, tuple) and len(sample) == 4, f\"Dataset sample should be a tuple of 4 elements, but got {type(sample)} of length {len(sample)}.\"\n",
        "    print(\"✅ Sample is a tuple of 4 elements.\")\n",
        "    \n",
        "    en_tokens, en_mask, fin_tokens, fin_mask = sample\n",
        "    \n",
        "    # 3. Assert that all parts of the sample are tensors with the correct shape.\n",
        "    expected_shapes = [torch.Size([CONTEXT_LEN]), torch.Size([CONTEXT_LEN, CONTEXT_LEN]), torch.Size([CONTEXT_LEN]), torch.Size([CONTEXT_LEN, CONTEXT_LEN])]\n",
        "    checks=[(\"English tokens\", en_tokens), (\"English mask\", en_mask), (\"Finnish tokens\", fin_tokens), (\"Finnish mask\", fin_mask)]\n",
        "    for i, (name, tensor) in enumerate(checks):\n",
        "        assert isinstance(tensor, torch.Tensor), f\"{name} is not a torch.Tensor.\"\n",
        "        assert tensor.shape == expected_shapes[i], (\n",
        "            f\"{name} shape is incorrect.\\n\"\n",
        "            f\"Expected: {expected_shapes[i]}, Got: {tensor.shape}\"\n",
        "        )\n",
        "    print(\"✅ All sample tensors have the correct shapes.\")\n",
        "    print(\"--- Sanity Check Passed ---\\n\")\n",
        "\n",
        "\n",
        "def analyze_dataset(dataset: EnFinnishDataset):\n",
        "    \"\"\"\n",
        "    Iterates through the entire dataset to collect token statistics and\n",
        "    verify that the tokenization process is reversible.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Full Dataset Analysis ---\")\n",
        "    en_token_counts = Counter()\n",
        "    fi_token_counts = Counter()\n",
        "    mismatched_decodings = []\n",
        "\n",
        "    # Use tqdm for a progress bar\n",
        "    for i in tqdm(range(len(dataset)), desc=\"Analyzing Dataset\"):\n",
        "        # 1. Get original text and tokenized tensors\n",
        "        original_en = dataset.english_corpus[i].strip()\n",
        "        original_fi = dataset.finnish_corpus[i].strip()\n",
        "        en_tokens, _, fin_tokens, _ = dataset[i]\n",
        "\n",
        "        # 2. Update token frequency counts\n",
        "        # We only count non-padding tokens to get meaningful statistics\n",
        "        en_non_pad_tokens = en_tokens[en_tokens != dataset.tokenizer.pad_token_id]\n",
        "        fi_non_pad_tokens = fin_tokens[fin_tokens != dataset.tokenizer.pad_token_id]\n",
        "        en_token_counts.update(en_non_pad_tokens.tolist())\n",
        "        fi_token_counts.update(fi_non_pad_tokens.tolist())\n",
        "\n",
        "        # 3. Check for information loss by decoding\n",
        "        # skip_special_tokens=True removes <pad> tokens from the output\n",
        "        decoded_en = dataset.tokenizer.decode(en_non_pad_tokens, skip_special_tokens=True).strip()\n",
        "        decoded_fi = dataset.tokenizer.decode(fi_non_pad_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        # Compare original with decoded. Some minor differences are expected\n",
        "        # due to tokenizer normalization, but major ones should be flagged.\n",
        "        if original_en != decoded_en:\n",
        "            mismatched_decodings.append((\"en\", i, original_en, decoded_en))\n",
        "        if original_fi != decoded_fi:\n",
        "            mismatched_decodings.append((\"fi\", i, original_fi, decoded_fi))\n",
        "            \n",
        "    print(\"\\n--- Analysis Complete ---\")\n",
        "\n",
        "    # --- Report Token Statistics ---\n",
        "    print(\"\\n--- Token Distribution Statistics ---\")\n",
        "    print(\"\\nTop 15 Most Common English Tokens:\")\n",
        "    for token_id, count in en_token_counts.most_common(15):\n",
        "        token_str = dataset.tokenizer.decode([token_id])\n",
        "        print(f\"  - Token: '{token_str}' (ID: {token_id}) | Count: {count:,}\")\n",
        "\n",
        "    print(\"\\nTop 15 Most Common Finnish Tokens:\")\n",
        "    for token_id, count in fi_token_counts.most_common(15):\n",
        "        token_str = dataset.tokenizer.decode([token_id])\n",
        "        print(f\"  - Token: '{token_str}' (ID: {token_id}) | Count: {count:,}\")\n",
        "        \n",
        "    # --- Report Information Integrity Check ---\n",
        "    print(\"\\n\\n--- Tokenization Integrity Check ---\")\n",
        "    total_sentences = len(dataset) * 2\n",
        "    num_mismatches = len(mismatched_decodings)\n",
        "    print(f\"✅ Found {num_mismatches} mismatched decodings out of {total_sentences:,} total sentences.\")\n",
        "\n",
        "    if num_mismatches > 0:\n",
        "        print(\"\\nDisplaying up to 5 examples of mismatches:\")\n",
        "        print(\"Note: Minor differences (e.g., whitespace, normalization) are common.\")\n",
        "        for lang, index, original, decoded in mismatched_decodings[:5]:\n",
        "            print(\"-\" * 20)\n",
        "            print(f\"Language: {lang}, Index: {index}\")\n",
        "            print(f\"Original: '{original}'\")\n",
        "            print(f\"Decoded:  '{decoded}'\")\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "\n",
        "test_dataloading_and_item_shape()\n",
        "full_dataset = EnFinnishDataset(archive_path=ARCHIVE_PATH, context_len=CONTEXT_LEN)\n",
        "analyze_dataset(full_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
