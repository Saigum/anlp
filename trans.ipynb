{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import polars as po\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataChunk\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class datasetobj:\n",
    "    def __init__(self,df_path):\n",
    "        self.df = po.read_csv(df_path, has_header=False, new_columns=[\"en\", \"fr\"])\n",
    "        self.vocab_en = FreqDist()\n",
    "        self.vocab_fr = FreqDist()\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "    def __len__(self):\n",
    "        return(len(self.df))\n",
    "    def __getitem__(self,index):\n",
    "        en = self.tokenizer(self.df[index,0])[\"input_ids\"]\n",
    "        fr = self.tokenizer(self.df[index,0])[\"input_ids\"]\n",
    "        return en,fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnFrDataset(Dataset):\n",
    "    def __init__(self,ds:datasetobj):\n",
    "        super().__init__()\n",
    "        self.dataset = ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.height\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## learned positional_encoding, like in GPT-2\n",
    "class PositionalEncodings(nn.Module):\n",
    "    def __init__(self,max_seq_len:int,hidden_size:int):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=max_seq_len,embedding_dim=hidden_size)\n",
    "    def forward(self,positions):\n",
    "        ## positions would have to be a set of indices\n",
    "        return(self.pos_emb(positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class attnconfig:\n",
    "    query_dim:int\n",
    "    key_dim:int\n",
    "    value_dim:int\n",
    "    model_dim:int\n",
    "    n_heads:int\n",
    "    causal_mask:bool=False\n",
    "    \n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,config:attnconfig):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.ModuleList([nn.Linear(config.query_dim,config.model_dim//config.n_heads) for _ in range(config.n_heads)])\n",
    "        self.Wk = nn.ModuleList([nn.Linear(config.key_dim,config.model_dim//config.n_heads) for _ in range(config.n_heads)])\n",
    "        self.Wv = nn.ModuleList([nn.Linear(config.value_dim,config.model_dim//config.n_heads) for _ in range(config.n_heads)])\n",
    "        self.sf = nn.Softmax(dim=-1)\n",
    "        self.config=config\n",
    "    def forward(self,query_vector,key_vector,value_vector):\n",
    "        output =[]\n",
    "        for i in range(self.config.n_heads):\n",
    "            q=self.Wq[i](query_vector)\n",
    "            k=self.Wk[i](key_vector)\n",
    "            v=self.Wv[i](value_vector)\n",
    "            A = self.sf(torch.matmul(q,k.T)/(np.sqrt(1/q.shape[1])))\n",
    "            output.append(A@v)\n",
    "            # print(f\"Shape of head {i} output; {output[-1].shape}\")\n",
    "        return(torch.cat(output,dim=-1))\n",
    "\n",
    "class MaskedMultiHeadAttention(MultiHeadedAttention):\n",
    "    def forward(self,query_vector,key_vector,value_vector):\n",
    "        output =[]\n",
    "        for i in range(self.config.n_heads):\n",
    "            q=self.Wq[i](query_vector)\n",
    "            k=self.Wk[i](key_vector)\n",
    "            v=self.Wv[i](value_vector)\n",
    "            A = self.sf(torch.matmul(q,k.T)/(np.sqrt(1/q.shape[1])))\n",
    "            A= torch.tril(input=A)\n",
    "            output.append(A@v)\n",
    "            # print(f\"Shape of head {i} output; {output[-1].shape}\")\n",
    "        return(torch.cat(output,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResMLP(nn.Module):\n",
    "    def __init__(self, input_size:int,num_layers:int):\n",
    "        super().__init__()\n",
    "        self.Linears = nn.ModuleList([nn.Linear(input_size,input_size) for _ in range(num_layers)])\n",
    "    def forward(self,x):\n",
    "        res =x\n",
    "        for i in range(len(self.Linears)):\n",
    "            x = self.Linears[i](x)  \n",
    "        return res+x\n",
    "@dataclass\n",
    "class EncoderConfig:\n",
    "      num_heads:int=4\n",
    "      vocab_size:int=50762\n",
    "      embedding_size:int=768\n",
    "      max_seq_len:int=200\n",
    "      atn_cfg:attnconfig=attnconfig(query_dim=embedding_size,key_dim=embedding_size,value_dim=embedding_size,model_dim=embedding_size,n_heads=num_heads)\n",
    "      pos_weight:int=0.2\n",
    "      mlp_depth:int=1\n",
    "      \n",
    "      \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,config:EncoderConfig):\n",
    "        super().__init__()\n",
    "        self.Embedding = nn.Embedding(config.vocab_size,config.embedding_size)\n",
    "        self.PositionalEncoding =  PositionalEncodings(config.max_seq_len,config.embedding_size)\n",
    "        self.attn_head = MultiHeadedAttention(config=config.atn_cfg)\n",
    "        self.layer_norm1 = nn.LayerNorm(config.embedding_size)\n",
    "        self.res1 = ResMLP(input_size=config.embedding_size,num_layers=config.mlp_depth)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.embedding_size)\n",
    "        self.encodercfg = config\n",
    "    \n",
    "    def forward(self,x):\n",
    "        embs = self.Embedding(x)\n",
    "        pos_embs = self.PositionalEncoding(torch.arange(0,end=x.shape[0]))\n",
    "        embs = embs + self.encodercfg.pos_weight*pos_embs\n",
    "        embs = self.layer_norm1(self.attn_head(embs,embs,embs) + embs)\n",
    "        embs = self.layer_norm2(self.res1(embs))\n",
    "        return embs\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class DecoderConfig:\n",
    "    num_heads:int=4\n",
    "    vocab_size:int=50762\n",
    "    embedding_size:int=768\n",
    "    max_seq_len:int=200\n",
    "    atn_cfg:attnconfig=attnconfig(query_dim=embedding_size,key_dim=embedding_size,value_dim=embedding_size,model_dim=embedding_size,n_heads=num_heads)\n",
    "    pos_weight:int=0.2\n",
    "    mlp_depth:int=1\n",
    "        \n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,config:DecoderConfig):\n",
    "        super().__init__()\n",
    "        self.Embedding = nn.Embedding(config.vocab_size,config.embedding_size)\n",
    "        self.PositionalEncoding =  PositionalEncodings(config.max_seq_len,config.embedding_size)\n",
    "        self.attn_head = MaskedMultiHeadAttention(config.atn_cfg)\n",
    "        self.res1 = nn.Sequential( ResMLP(input_size=config.embedding_size,num_layers=config.mlp_depth),\n",
    "                                  nn.LayerNorm(config.embedding_size))\n",
    "        self.encoder_inputs = MultiHeadedAttention(config.atn_cfg)\n",
    "        self.res2 = nn.Sequential(ResMLP(input_size=config.embedding_size,num_layers=config.mlp_depth),\n",
    "                                    nn.LayerNorm(config.embedding_size))\n",
    "        self.res3 =  self.res2 = nn.Sequential(ResMLP(input_size=config.embedding_size,num_layers=config.mlp_depth),\n",
    "                                    nn.LayerNorm(config.embedding_size))\n",
    "        self.fc = nn.Sequential(nn.Linear(config.embedding_size,config.vocab_size),\n",
    "                                nn.Softmax(-1))\n",
    "        \n",
    "        self.decodercfg = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my mha op shape: torch.Size([5, 20])\n",
      "This is torch mha output shape torch.Size([5, 20])\n",
      "This is my mha op shape: torch.Size([5, 20])\n",
      "This is torch mha output shape torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multihead_config = attnconfig(query_dim=20,key_dim=20,value_dim=20,model_dim=20,n_heads=2)\n",
    "selfattn_config = attnconfig(query_dim=20,key_dim=20,value_dim=20,model_dim=20,n_heads=1)\n",
    "\n",
    "def test_mha(config:attnconfig=selfattn_config):\n",
    "    mha_layer = MultiHeadedAttention(config)\n",
    "    torch_mha = nn.MultiheadAttention(config.model_dim,config.n_heads)\n",
    "    N_vectors = 5\n",
    "    query_vector = torch.randn([N_vectors,config.query_dim])\n",
    "    key_vector = torch.randn([N_vectors,config.key_dim])\n",
    "    value_vector = torch.randn([N_vectors,config.value_dim])\n",
    "    with torch.no_grad():\n",
    "        output = mha_layer(query_vector,key_vector,value_vector)\n",
    "        torchput,attn_weights = torch_mha(query_vector,key_vector,value_vector)\n",
    "    print(f\"This is my mha op shape: {output.shape}\") ## should be n x value vector size\n",
    "    print(f\"This is torch mha output shape {torchput.shape}\")\n",
    "\n",
    "test_mha(selfattn_config)\n",
    "test_mha(multihead_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "encfg=EncoderConfig(num_heads=4,vocab_size=50762,embedding_size=768,max_seq_len=2000,pos_weight=0.2)\n",
    "def test_encoder(encodercfg:EncoderConfig):\n",
    "    trans = TransformerEncoder(encodercfg)\n",
    "    batch_size=10\n",
    "    input_tokens = torch.randint(low=0,high=encodercfg.vocab_size,size=(batch_size,))\n",
    "    with torch.no_grad():\n",
    "        output = trans(input_tokens)\n",
    "    print(output.shape)\n",
    "test_encoder(encfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishFrenchDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
